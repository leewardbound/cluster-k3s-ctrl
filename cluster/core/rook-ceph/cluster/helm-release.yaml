---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.8.5
      sourceRef:
        kind: HelmRepository
        name: rook-ceph-charts
        namespace: flux-system
  install:
    createNamespace: true
    remediation:
      retries: 3
  upgrade:
    remediation:
      retries: 3
  dependsOn:
    - name: rook-ceph
      namespace: rook-ceph
  values:
    monitoring:
      enabled: true
    ingress:
      dashboard:
        ingressClassName: nginx-private
        annotations:
          kubernetes.io/ingress.class: "nginx"
          hajimari.io/enable: "true"
        host:
          name: &host "rook.${SECRET_PRIVATE_DOMAIN}"
          path: "/"
        tls:
          - hosts:
              - *host
    cephClusterSpec:
      placement:
        all:
          tolerations:
            - effect: NoSchedule
              operator: Exists
            - effect: NoExecute
              operator: Exists
      crashCollector:
        disable: false
      mgr:
        count: 2
      dashboard:
        enabled: true
        urlPrefix: /
        ssl: false
      storage:
        useAllNodes: false
        useAllDevices: false
        config:
          osdsPerDevice: "1"
        nodes:
          - name: "enterprise"
            devices:
              - name: "sdb"
          - name: "vega"
            devices:
              - name: "sdb"
              - name: "sdc"
          - name: "ziti"
            devices:
              - name: "sda"
              - name: "sdc"
    cephBlockPools:
      - name: ceph-blockpool
        spec:
          failureDomain: host
          replicated:
            size: 2
        storageClass:
          enabled: true
          name: ceph-block
          isDefault: true
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            imageFormat: "2"
            imageFeatures: layering
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph
            csi.storage.k8s.io/fstype: ext4
    cephFileSystems:
      - name: cephfs-hdd
        spec:
          preserveFilesystemOnDelete: false
          metadataServer:
            activeCount: 1
            activeStandby: true
          metadataPool:
            deviceClass: hdd
            replicated:
              size: 2
              requireSafeReplicaSize: true
            parameters:
              # Inline compression mode for the data pool
              # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
              compression_mode: none
              # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
              # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
              target_size_ratio: ".5"
          # The list of data pool specs. Can use replication or erasure coding.
          dataPools:
            - name: data0
              failureDomain: host
              deviceClass: hdd
              replicated:
                size: 2
                # Disallow setting pool with replica 1, this could lead to data loss without recovery.
                # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
                requireSafeReplicaSize: true
              parameters:
                # Inline compression mode for the data pool
                # Further reference: https://docs.ceph.com/docs/master/rados/configuration/bluestore-config-ref/#inline-compression
                compression_mode: none
                # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
                # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
                target_size_ratio: ".5"
        storageClass:
          enabled: true
          name: cephfs-hdd
          isDefault: false
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: rook-ceph
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: rook-ceph
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: rook-ceph

    cephObjectStores: []
